
gate+game服测试：
. 先启动etcd服务器和nats服务器，cluster_test_1中,etcd和nats都只启动一个，集群部署后面再看
. 启动gate服
. 启动game服
. 启动client
. 然后查看运行结果


注:
. 不同系统的release包都可以在github上下载
. etcd 和 nats的windows包我放在bin目录下了!!!
. gate服与game服都使用cluster模式
. gate服与game服不直接连接，通过nats消息队列做转发

. client到gate的消息，如果配置的处理协程数量>1,那么对于单个玩家的消息有可能乱序，因此我对pitaya的这部分处理有所修改(agentImpl中增加了chProcess)。
. 同理，通过nats处理的远程消息，如果配置的处理协程数量>1，也是多协程处理，这里可以根据具体的场景去处理，比如如果是大地图服务器/公会服务器，可以考虑把协程数量配置为1。
  原处理在nats_rpc_server.go的processMessages函数，继续跟进，可以发现ns.pitayaServer.Call调用的是remote.go的RemoteService.Call, 里面把实际的处理又放在一个
  新创建的协程中:
      		go func() {
      			result <- processRemoteMessage(c, req, r)
      		}()
  如果要保证消息的处理顺序则必须对这部分逻辑进行处理

. nats : 消息队列，处理消息的 发布/订阅  请求/回应 等模式，请求/回应 模式其实是由 发布/订阅 实现的。
         比如，client发送一个消息需要game服处理，则流程如下：

             . client发送消息到gate

             . gate发送消息到nats，然后阻塞等待nats回应消息（pitaya使用的是nats的 请求/回应模型: gate 调用 remoteCall，然后通过
               NatsRPCClient 的 Call 函数 Publish 消息到nats，发布时需要使用etcd client提前获取到的目标服务器的相关信息，只有这样
               NatsRPCServer 才能订阅到gate服发布的消息。具体实现可以查看 remoteCall 函数实现。
               比如这一句：target, err = r.router.Route(ctx, rpcType, svType, route, msg); // 获取目标服相关信息
               随后nats会分发到相应的服务器。gate阻塞等待nats的回应，pitaya默认开了几十个协程来处理请求消息，因此不用担心会卡住系统）

             . nats发送消息到game，game处理消息后返回处理结果给nats(game服使用 NatsRPCServer 的 processMessages 函数来处理nats发过来的消息，
               因为nats rpc client使用的是请求/回应模型, 所以game服的nats rpc server 先使用 subscribe 里的 ChanSubscribe 订阅来接收nats发送
               过来的消息，处理完消息后使用nats的 Publish 将消息从game服发布到nats服务器，nats服务器再返回消息给gate服。所以综上可以看出
               请求/回应 是由 nats rpc client: 发布+订阅 和 nats rpc server: 订阅+发布 来实现的)

             . nats回应消息，gate收到回应

             . gate返回消息给client，流程结束.


. etcd : 服务注册与发现。通俗的讲应该是：当我们新部署一个game服时，
         其它服务器知道新game服的存在，新增加一个gate服是也如此
         当一个服务器挂掉，其它服务器也会立刻知道有服务器挂了，当然
         我们可以自己写一个类似的服务器XX来管理 新增/关闭/挂掉 服务器的
         相应管理工作，但是会很麻烦。因为cluster帮我们做好来这些，并且
         etcd支持分布式一致性！！！这点很重要！当我们集群部署3个etcd服时，
         每个etcd服上的数据都是一致的，当其中一个etcd服挂掉时，其它两个还
         能正常工作，不影响系统运行。

         etcd使用raft算法实现分布式一致性
         我们可以使用etcd做 服务注册&发现/分布式锁/共享配置/负载均衡 等等操作

   问题:
         使用的时候明明是Put的 servers/*** 但是用etcdctl查看key却是 pitaya/servers/*
         原来它使用来 etcdPrefix 来处理了一下...

         nats 使用docker部署在虚拟机上，virtual box做了端口映射4222，但是windows上始终无法connect 4222，在linux上却可以
         后来把端口映射改成42221->4222就行了，应该是windows上对4222做了特殊处理，比如保留了或者防火墙过滤了等原因...


扩展：

nats:
    NATS的消息通信是这样的：应用程序的数据被编码为一条消息，并通过发布者发送出去；订阅者接收到消息，进行解码，再处理。订阅者处理NATS消息可以是同步的或异步的。

    * 异步处理
    　　异步处理使用回调消息句柄处理消息，当有消息到来时，已注册的回调句柄接收并控制处理消息。整个过程客户端不会被阻塞，可以同步执行其它任务。异步处理可以采用多线程调度的设计。
    * 同步处理
    　　同步处理需要应用程序显示调用方法来处理到来的消息。这种显示调用是阻塞式的调用，会暂停任务直到消息可用。如果没有可用的消息，消息处理阻塞的周期由客户端设置。同步处理通常用于服务器等待并处理传入的请求消息，并发送响应给客户端。


    NATS支持以下三种消息通信模型：

    1. 发布/订阅模型
    　　NATS的发布/订阅通信模型是一对多的消息通信。发布者在一个主题上发送消息，任何注册（订阅）了此主题的客户端都可以接收到该主题的消息。订阅者可以使用主题通配符订阅感兴趣的主题。
    　　对于订阅者，可以选择异步处理或同步处理接收到的消息。如果异步处理消息，消息交付给订阅者的消息句柄。如果客户端没有句柄，那么该消息通信是同步的，那么客户端可能会被阻塞，直到它处理了当前消息。

    　　服务质量（QoS）
    　　至多发送一次 (TCP reliability)：如果客户端没有注册某个主题（或者客户端不在线），那么该主题发布消息时，客户端不会收到该消息。NATS系统是一种“发送后不管”的消息通信系统，故如果需要高级服务，可以选择"NATS Streaming" 或 在客户端开发相应的功能
    　　至少发送一次(NATS Streaming) ：一些使用场景需要更高级更严格的发送保证，这些应用依赖底层传送消息，不论网络是否中断或订阅者是否在线，都要确保订阅者可以收到消息


    2. 请求/响应模型
    　　NATS支持两种请求-响应消息通信：P2P（点对点）和O2M（一对多）。P2P最快、响应也最先。而对于O2M，需要设置请求者可以接收到的响应数量界限（默认只能收到一条来自订阅者的响应——随机）
    在请求-响应模式，发布请求操作会发布一个带预期响应的消息到Reply主题。
    请求创建了一个收件箱，并在收件箱执行调用，并进行响应和返回

    多个订阅者(reply 例子)订阅了同一个 主题，请求者向该主题发送一个请求，默认只收到一个订阅者的响应(随机)

    事实上，NATS协议中并没有定义 “请求” 或 "响应"方法，它是通过 SUB/PUB变相实现的：请求者先 通过SUB创建一个收件箱，然后发送一个带 reply-to 的PUB，响应者收到PUB消息后，向 reply-to 发送 响应消息，从而实现 请求/响应。reply-to和收件箱都是一个 subject，前者是后者的子集（O2M的情况）


    3. 队列模型
    　　NATS支持P2P消息通信的队列。要创建一个消息队列，订阅者需注册一个队列名。所有的订阅者用同一个队列名，形成一个队列组。当消息发送到主题后，队列组会自动选择一个成员接收消息。尽管队列组有多个订阅者，但每条消息只能被组中的一个订阅者接收。
    队列的订阅者可以是异步的，这意味着消息句柄以回调方式处理交付的消息。同步队列订阅者必须建立处理消息的逻辑

    　　NATS支持P2P消息通信的队列。要创建一个消息队列，订阅者需注册一个队列名。所有的订阅者用同一个队列名，形成一个队列组。当消息发送到主题后，队列组会自动选择一个成员接收消息。尽管队列组有多个订阅者，但每条消息只能被组中的一个订阅者接收。
    　　队列的订阅者可以是异步的，这意味着消息句柄以回调方式处理交付的消息。异步队列订阅者必须建立处理消息的逻辑。

    　　队列模型一般常用于数据队列使用，例如：从网页上采集的数据经过处理直接写入到该队列，接收端一方可以起多个线程同时读取其中的一个队列，其中某些数据被一个线程消费了，其他线程就看不到了，这种方式为了解决采集量巨大的情况下，后端服务可以动态调整并发数来消费这些数据。说白了就一点，上游生产数据太快，下游消费可能处理不过来，中间进行缓冲，下游就可以根据实际情况进行动态调整达到动态平衡。


    NATS特性
    　　NATS提供了以下独特的功能：
    　　1）纯发布/订阅
    　　　　永远不假定有接收者
    　　　　总是在线
    　　2）集群模式的服务器
    　　　　NATS服务器可以集群；
    　　　　发布式的队列可以跨域集群；
    　　　　集群感知的客户端
    　　3）订阅者的自动修剪
    　　　　要支持可伸缩性，NATS提供了客户端连接的自动修剪功能；
    　　　　如果某个客户端APP处理消息很慢，NATS会自动关闭此客户端的连接；
    　　　　如果某个客户端在ping-pong时间间隔内未做响应，服务器会自动关闭此连接；
    　　　　客户端实现重连逻辑
    　　4）基于文本的协议
    　　　　开发上手比较容易；
    　　　　不影响服务器的性能；
    　　　　可以直接用Telnet连接服务器
    　　5）多种 QoS
    　　　　至多发送一次（TCP level reliability）---NATS立即向符合条件的订阅者发送消息，并不存留消息
    　　　　https://www.zhihu.com/question/49596182
    　　　　至少发送一次（via NATS Streaming）--- 如果匹配的订阅者一时不在线，Message 将被存储直到它被传送给订阅者，并得到订阅者确认。除非 该消息超时或存储空间耗尽
    　　6）持久性订阅（via NATS Streaming）
    　　　　服务端维护 持久性订阅者的 订阅推送状态，这样，持久性订阅者就可以知道它们在上一次会话中是在哪儿断开的
    　　7）Event 流服务(via NATS Streaming)
    　　　　根据时间戳、序列号或相对位差，消息被持久化存储在 内存、文件或其它二级存储设备中
    　　8）缓存 最新一个或第一个值 (via NATS Streaming)
    　　　　订阅者连接上服务器以后，先向订阅者推送最近一次的publish消息